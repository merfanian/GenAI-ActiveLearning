import json
import logging
import shutil
from pathlib import Path
import sys
import os

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..")))

import numpy as np
from services import (
    data_service,
    model_service,
    fairness_service,
    llm_augmentation_service,
)
from utils.config import AUGMENTED_IMAGES_DIR, TRAINED_MODELS_DIR
from utils.logging_config import LOGGING_CONFIG
from dotenv import load_dotenv

from utils.attribute_mappings import FRUITS_LABEL_MAPPING, FRUITS_LABEL_TARGET, FFHQ_GENDER_MAPPING, FFHQ_GENDER_TARGET, ANIMALS_LABEL_MAPPING, ANIMALS_LABEL_TARGET

load_dotenv("/home/mahdi/Projects/GenAI-ActiveLearning/.env")

ATTRIBUTE_VALUE_MAPPING = ANIMALS_LABEL_MAPPING

TARGET_LABEL_MAPPING = ANIMALS_LABEL_TARGET

# --- Configuration ---
RESULTS_DIR = Path("results")
RESULTS_DIR.mkdir(exist_ok=True)
MASK_DIR = Path("masks")
MASK_DIR.mkdir(exist_ok=True)

IMAGE_DIR_PATH = "/home/mahdi/Projects/GenAI-ActiveLearning/resources/animals"
METADATA_CSV_PATH = "/home/mahdi/Projects/GenAI-ActiveLearning/resources/animals/metadata_4000.csv"

TARGET_ATTRIBUTE = "label"
FAIRNESS_ATTRIBUTE = "label"
INITIAL_MODEL_NAME = "model.pth"
ARCHITECTURE = "resnet"
NUM_ITERATIONS = 10
AUGMENTATION_BATCH_SIZE = 10
ACCURACY_THRESHOLD = 0.95
NUM_RUNS = 3


def run_exploration_ablation(run_number: int):
    # --- 1. Setup Dataset ---
    logging.info("Setting up the dataset...")
    data_service.load_and_validate_dataset(
        IMAGE_DIR_PATH, METADATA_CSV_PATH, TARGET_ATTRIBUTE, FAIRNESS_ATTRIBUTE
    )

    # --- 2. Train Initial Model ---
    logging.info("Checking for an existing initial model...")
    initial_model_path = TRAINED_MODELS_DIR / INITIAL_MODEL_NAME
    if not initial_model_path.exists():
        logging.info("No existing model found, training a new one...")
        image_paths, labels, _ = data_service.get_train_val_image_paths_and_labels(
            include_augmented=False
        )
        initial_model_path = model_service.train_model(
            image_paths,
            labels,
            architecture=ARCHITECTURE,
            updated_model_path=INITIAL_MODEL_NAME,
        )
        logging.info(f"Initial model trained and saved to {initial_model_path}")
    else:
        logging.info(
            f"Found existing model at {initial_model_path}, skipping training."
        )

    model_service.set_current_model_path(str(initial_model_path))

    # --- 3. Define Ablation Scenarios ---
    scenarios = {
        "exploitation_only": {"exploration_mode": "exploitation_only"},
        "exploration_only": {"exploration_mode": "exploration_only"},
        "balanced": {"exploration_mode": "balanced"},
    }

    # --- 4. Run Ablation Study ---
    for scenario_name, params in scenarios.items():
        logging.info(f"--- Running Scenario: {scenario_name} for Run: {run_number} ---")

        scenario_run_augmented_dir = (
            AUGMENTED_IMAGES_DIR / f"{scenario_name}_run_{run_number}"
        )
        if scenario_run_augmented_dir.exists():
            shutil.rmtree(scenario_run_augmented_dir)
        scenario_run_augmented_dir.mkdir(parents=True)

        # Reset to the initial model for each scenario
        scenario_model_path = (
            TRAINED_MODELS_DIR / f"{scenario_name}_run_{run_number}_model.pth"
        )
        shutil.copy(initial_model_path, scenario_model_path)
        model_service.set_current_model_path(str(scenario_model_path))

        scenario_results = []

        for i in range(NUM_ITERATIONS):
            logging.info(
                f"--- Iteration {i + 1}/{NUM_ITERATIONS} for {scenario_name} (Run {run_number}) ---"
            )

            # Evaluate fairness
            df = data_service.get_test_metadata_df()
            image_paths, labels = data_service.get_test_image_paths_and_labels()
            gp_raw = fairness_service.calculate_group_performances(
                str(scenario_model_path), df, image_paths, labels, FAIRNESS_ATTRIBUTE
            )
            worst_raw = fairness_service.find_worst_performing_group(gp_raw)
            worst_acc = worst_raw.get("accuracy", 0.0)

            if worst_acc >= ACCURACY_THRESHOLD:
                logging.info("Accuracy threshold met. Stopping scenario.")
                break

            # Augment
            generated = llm_augmentation_service.generate_image_and_get_label(
                worst_raw["attributes"],
                AUGMENTATION_BATCH_SIZE,
                exploration_mode=params["exploration_mode"],
                augmented_data_dir=scenario_run_augmented_dir,
                attribute_mapping=ATTRIBUTE_VALUE_MAPPING,
                target_label_mapping=TARGET_LABEL_MAPPING,
            )

            for item in generated:
                data_service.add_augmented_data(
                    item["filename"],
                    item["attributes_used"],
                    item["llm_acquired_label"],
                    augmented_data_dir=scenario_run_augmented_dir,
                )

            # Retrain
            all_image_paths, all_labels, _ = (
                data_service.get_train_val_image_paths_and_labels(
                    include_augmented=True,
                    augmented_data_dir=scenario_run_augmented_dir,
                )
            )
            new_model_path = model_service.train_model(
                all_image_paths,
                all_labels,
                existing_model_path=str(scenario_model_path),
                updated_model_path=f"{scenario_name}_run_{run_number}_model_iter_{i + 1}.pth",
            )

            # Evaluate the new model
            new_gp_raw = fairness_service.calculate_group_performances(
                str(new_model_path), df, image_paths, labels, FAIRNESS_ATTRIBUTE
            )
            new_worst_raw = fairness_service.find_worst_performing_group(new_gp_raw)
            new_worst_acc = new_worst_raw.get("accuracy", 0.0)

            # Check for performance degradation
            if new_worst_acc < worst_acc:
                logging.warning(
                    f"Performance degraded after augmentation (before: {worst_acc:.4f}, after: {new_worst_acc:.4f}). "
                    f"Discarding model and rolling back data."
                )
                data_service.remove_last_augmented_batch(
                    len(generated), augmented_data_dir=scenario_run_augmented_dir
                )
                new_worst_raw = worst_raw
            else:
                model_service.set_current_model_path(str(new_model_path))
                scenario_model_path = new_model_path

            # Log results
            overall_accuracy = fairness_service.calculate_overall_accuracy(
                str(scenario_model_path), image_paths, labels
            )
            logging.info(
                f"Scenario: {scenario_name}, Iteration: {i + 1}, "
                f"Worst Group Accuracy: {new_worst_raw.get('accuracy', 0.0):.4f}, "
                f"Overall Accuracy: {overall_accuracy:.4f}"
            )

            scenario_results.append(
                {
                    "iteration": i + 1,
                    "worst_group_before_aug": worst_raw,
                    "worst_group_after_aug": new_worst_raw,
                    "overall_accuracy": overall_accuracy,
                    "augmented_images_generated": len(generated),
                    "model_path": str(scenario_model_path),
                }
            )

        # --- Calculate Price of Fairness, Save and Cleanup after scenario ---
        initial_test_image_paths, initial_test_labels = (
            data_service.get_test_image_paths_and_labels()
        )
        initial_accuracy = fairness_service.calculate_overall_accuracy(
            str(initial_model_path), initial_test_image_paths, initial_test_labels
        )
        final_accuracy = fairness_service.calculate_overall_accuracy(
            str(scenario_model_path), initial_test_image_paths, initial_test_labels
        )
        price_of_fairness = initial_accuracy - final_accuracy
        logging.info(f"Price of Fairness for {scenario_name}: {price_of_fairness:.4f}")

        scenario_summary = {
            "run_number": run_number,
            "scenario_name": scenario_name,
            "parameters": params,
            "initial_model_path": str(initial_model_path),
            "final_model_path": str(scenario_model_path),
            "initial_accuracy": initial_accuracy,
            "final_accuracy": final_accuracy,
            "price_of_fairness": price_of_fairness,
            "iterations": scenario_results,
        }

        results_path = RESULTS_DIR / f"{scenario_name}_run_{run_number}_results.json"
        with open(results_path, "w") as f:
            json.dump(scenario_summary, f, indent=4)
        logging.info(
            f"Saved results for scenario '{scenario_name}' (Run {run_number}) to {results_path}"
        )

        logging.info(
            f"--- Cleaning up after scenario: {scenario_name} (Run {run_number}) ---"
        )
        if scenario_run_augmented_dir.exists():
            shutil.rmtree(scenario_run_augmented_dir)
            logging.info(
                f"Removed augmented images directory: {scenario_run_augmented_dir}"
            )


def main():
    for run in range(1, NUM_RUNS + 1):
        logging.info(f"========== STARTING RUN {run}/{NUM_RUNS} ==========")
        run_exploration_ablation(run)
        logging.info(f"========== COMPLETED RUN {run}/{NUM_RUNS} ==========")


if __name__ == "__main__":
    main()
